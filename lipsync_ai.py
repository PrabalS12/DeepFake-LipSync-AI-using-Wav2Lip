# -*- coding: utf-8 -*-
"""LipSync AI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xo2Ta31WS5FH0ZTRpOMjlcCY3kAv0bxX

**Google Drive Link to the Demo Video Solution Attached:**

https://drive.google.com/file/d/1Z5ze-4a7EydHpySTr85tFbtDWBQePG_u/view?usp=sharing
"""

# Commented out IPython magic to ensure Python compatibility.
#@title <h1>Step1: Setup Wav2Lip</h1>
#@markdown <b> Run this cell, it will --  </b>
#@markdown * Install dependency
#@markdown * Download pretrained model
#@markdown *This includes Cloning git repository, Downloading Pre trained Wav2Lip GAN model, & Pre Trained Face Detection Model, and a few functions
!rm -rf /content/sample_data
!mkdir /content/sample_data

!git clone https://github.com/justinjohn0306/Wav2Lip

#download the pretrained model
!wget 'https://iiitaphyd-my.sharepoint.com/personal/radrabha_m_research_iiit_ac_in/_layouts/15/download.aspx?share=EdjI7bZlgApMqsVoEUUXpLsBxqXbn5z8VTmoxp55YNDcIA' -O '/content/Wav2Lip/checkpoints/wav2lip_gan.pth'
a = !pip install https://raw.githubusercontent.com/AwaleSajil/ghc/master/ghc-1.0-py3-none-any.whl

# !pip uninstall tensorflow tensorflow-gpu
!cd Wav2Lip && pip install -r requirements.txt

#download pretrained model for face detection
!wget "https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth" -O "/content/Wav2Lip/face_detection/detection/sfd/s3fd.pth"

!pip install ffmpeg-python

#this code for recording audio
"""
To write this piece of code I took inspiration/code from a lot of places.
It was late night, so I'm not sure how much I created or just copied o.O
Here are some of the possible references:
https://blog.addpipe.com/recording-audio-in-the-browser-using-pure-html5-and-minimal-javascript/
https://stackoverflow.com/a/18650249
https://hacks.mozilla.org/2014/06/easy-audio-capture-with-the-mediarecorder-api/
https://air.ghost.io/recording-to-an-audio-file-using-html5-and-js/
https://stackoverflow.com/a/49019356
"""
from IPython.display import HTML, Audio
from google.colab.output import eval_js
from base64 import b64decode
import numpy as np
from scipy.io.wavfile import read as wav_read
import io
import ffmpeg

AUDIO_HTML = """
<script>
var my_div = document.createElement("DIV");
var my_p = document.createElement("P");
var my_btn = document.createElement("BUTTON");
var t = document.createTextNode("Press to start recording");

my_btn.appendChild(t);
//my_p.appendChild(my_btn);
my_div.appendChild(my_btn);
document.body.appendChild(my_div);

var base64data = 0;
var reader;
var recorder, gumStream;
var recordButton = my_btn;

var handleSuccess = function(stream) {
  gumStream = stream;
  var options = {
    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k
    mimeType : 'audio/webm;codecs=opus'
    //mimeType : 'audio/webm;codecs=pcm'
  };
  //recorder = new MediaRecorder(stream, options);
  recorder = new MediaRecorder(stream);
  recorder.ondataavailable = function(e) {
    var url = URL.createObjectURL(e.data);
    var preview = document.createElement('audio');
    preview.controls = true;
    preview.src = url;
    document.body.appendChild(preview);

    reader = new FileReader();
    reader.readAsDataURL(e.data);
    reader.onloadend = function() {
      base64data = reader.result;
      //console.log("Inside FileReader:" + base64data);
    }
  };
  recorder.start();
  };

recordButton.innerText = "Recording... press to stop";

navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);


function toggleRecording() {
  if (recorder && recorder.state == "recording") {
      recorder.stop();
      gumStream.getAudioTracks()[0].stop();
      recordButton.innerText = "Saving the recording... pls wait!"
  }
}

// https://stackoverflow.com/a/951057
function sleep(ms) {
  return new Promise(resolve => setTimeout(resolve, ms));
}

var data = new Promise(resolve=>{
//recordButton.addEventListener("click", toggleRecording);
recordButton.onclick = ()=>{
toggleRecording()

sleep(2000).then(() => {
  // wait 2000ms for the data to be available...
  // ideally this should use something like await...
  //console.log("Inside data:" + base64data)
  resolve(base64data.toString())

});

}
});

</script>
"""

# %cd /
from ghc.l_ghc_cf import l_ghc_cf
# %cd content

def get_audio():
  display(HTML(AUDIO_HTML))
  data = eval_js("data")
  binary = b64decode(data.split(',')[1])

  process = (ffmpeg
    .input('pipe:0')
    .output('pipe:1', format='wav')
    .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)
  )
  output, err = process.communicate(input=binary)

  riff_chunk_size = len(output) - 8
  # Break up the chunk size into four bytes, held in b.
  q = riff_chunk_size
  b = []
  for i in range(4):
      q, r = divmod(q, 256)
      b.append(r)

  # Replace bytes 4:8 in proc.stdout with the actual size of the RIFF chunk.
  riff = output[:4] + bytes(b) + output[8:]

  sr, audio = wav_read(io.BytesIO(riff))

  return audio, sr


from IPython.display import HTML
from base64 import b64encode
def showVideo(path):
  mp4 = open(str(path),'rb').read()
  data_url = "data:video/mp4;base64," + b64encode(mp4).decode()
  return HTML("""
  <video width=700 controls>
        <source src="%s" type="video/mp4">
  </video>
  """ % data_url)

from IPython.display import clear_output

import os
import shutil

# Define the path to custom inference.py file
custom_inference_path = '/content/inference.py'

# Define the path to the Wav2Lip inference file directory
wav2lip_inf_directory = '/content/Wav2Lip/temp'

# Check if directory exists
if not os.path.exists(wav2lip_inf_directory):
    os.makedirs(wav2lip_inf_directory)

# Remove the existing inference.py file
existing_inference_path = os.path.join(wav2lip_inf_directory, 'inference.py')
if os.path.exists(existing_inference_path):
    os.remove(existing_inference_path)
    print("Existing inference.py file removed.")

# Copy your custom inference.py file to the Wav2Lip directory
shutil.copyfile(custom_inference_path, existing_inference_path)
print("Custom inference.py file copied.")

# Now you can run the Wav2Lip inference using your custom inference.py file
# ... (Add your inference code here)

#@title STEP 2: Select a Youtube Video
# Install yt-dlp

import os
!pip install yt-dlp

#@markdown ## Find YouTube video ID from URL

#@markdown ___

#@markdown Link format:

#@markdown ``https://youtu.be/vAnWYLTdvfY`` ❌

#@markdown ``https://www.youtube.com/watch?v=D1yk7sXBKVA`` ✅

def get_video_resolution(video_path):
    """Function to get the resolution of a video"""
    import cv2
    video = cv2.VideoCapture(video_path)
    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))
    return (width, height)

!rm -df youtube.mp4

#@markdown ___
from urllib import parse as urlparse
YOUTUBE_URL = 'https://www.youtube.com/watch?v=D1yk7sXBKVA' #@param {type:"string"}
url_data = urlparse.urlparse(YOUTUBE_URL)
query = urlparse.parse_qs(url_data.query)
YOUTUBE_ID = query["v"][0]


# remove previous input video
!rm -f /content/sample_data/input_vid.mp4


#@markdown ___

#@markdown ### Trim the video (start, end) seconds
start = 0 #@param {type:"integer"}
end = 67 #@param {type:"integer"}
interval = end - start

#@markdown <font color="orange"> Note: ``the trimmed video must have face on all frames``

# Download the YouTube video using yt-dlp
!yt-dlp -f 'bestvideo[ext=mp4]' --output "youtube.%(ext)s" https://www.youtube.com/watch?v=$YOUTUBE_ID

#@markdown <br><br>
#@markdown * Original Video saved at '../sample_data/input_vid.mp4'
#@markdown * The video displayed under is Sped Up for Wav2Lip Model to work/handle large video files.
# Cut the video using FFmpeg
!ffmpeg -y -i youtube.mp4 -ss {start} -t {interval} -async 1 /content/sample_data/input_vid.mp4

# 2X the speed of my video(1.7 mins) to match the audio length(33 secs)
!ffmpeg -i /content/sample_data/input_vid.mp4 -filter:v "setpts=0.5*PTS" -c:v libx264 -preset veryfast -crf 18 -c:a copy /content/sample_data/input_vid_fast.mp4


PATH_TO_YOUR_VIDEO = '/content/sample_data/input_vid_fast.mp4'
video_resolution = get_video_resolution(PATH_TO_YOUR_VIDEO)
print(f"Video resolution: {video_resolution}")
if video_resolution[0] >= 1920 or video_resolution[1] >= 1080:
    print("Resizing video to 720p...")
    os.system(f"ffmpeg -i {PATH_TO_YOUR_VIDEO} -vf scale=1280:720 /content/sample_data/resized_video.mp4")
    PATH_TO_YOUR_VIDEO = "/content/sample_data/resized_video.mp4"
    print("Video resized to 720p")
else:
    print("No resizing needed")


# Preview the trimmed video
from IPython.display import HTML
from base64 import b64encode
mp4 = open(PATH_TO_YOUR_VIDEO,'rb').read()
data_url = "data:video/mp4;base64," + b64encode(mp4).decode()
HTML(f"""<video width=600 controls><source src="{data_url}"></video>""")

PATH_TO_YOUR_VIDEO

#@title STEP 3: Select Audio (Upload from local drive)
import os
from IPython.display import Audio
from IPython.core.display import display

upload_method = 'Upload' #@param ['Upload']

#remove previous input audio
if os.path.isfile('/content/sample_data/input_audio.wav'):
    os.remove('/content/sample_data/input_audio.wav')

def displayAudio():
  display(Audio('/content/sample_data/input_audio.wav'))

if upload_method == 'Upload':
  from google.colab import files
  uploaded = files.upload()
  for fn in uploaded.keys():
    print('User uploaded file "{name}" with length {length} bytes'.format(
        name=fn, length=len(uploaded[fn])))

  # Consider only the first file
  PATH_TO_YOUR_AUDIO = str(list(uploaded.keys())[0])

  # Use ffmpeg to speed up the audio by 2x
  !ffmpeg -i "{PATH_TO_YOUR_AUDIO}" -filter:a "atempo=2.0" "/content/sample_data/input_audio.wav"

#@markdown * To listen to the original audio - '../output10.wav'<br>
#@markdown * The audio displayed in the output is Sped Up for Wav2Lip to handle a video longer than a minute, while also taking lesser time.
#@markdown '
  clear_output()
  displayAudio()

#@title STEP 4: Start Crunching and Preview Output
#@markdown <b> Hyperparameter - Tune them carefully, these worked the best</b>

pad_top =  20#@param {type:"integer"}
pad_bottom =  6#@param {type:"integer"}
pad_left =  3#@param {type:"integer"}
pad_right =  0#@param {type:"integer"}
rescaleFactor = 1 #@param {type:"number"}
nosmooth = False #@param {type:"boolean"}
fps = 25 #@param {type:"integer"}
face_det_batch_size = 32 #@param {type:"integer"}
wav2lip_batch_size = 128 #@param {type:"integer"}


if nosmooth == False:
  !cd Wav2Lip && python inference.py --checkpoint_path checkpoints/wav2lip_gan.pth --face "{PATH_TO_YOUR_VIDEO}" --audio "../sample_data/input_audio.wav" --pads $pad_top $pad_bottom $pad_left $pad_right --resize_factor $rescaleFactor --fps $fps --face_det_batch_size $face_det_batch_size --wav2lip_batch_size $wav2lip_batch_size
else:
  !cd Wav2Lip && python inference.py \
  --checkpoint_path checkpoints/wav2lip_gan.pth \
  --face "{PATH_TO_YOUR_VIDEO}" \
  --audio "../sample_data/input_audio.wav" \
  --pads $pad_top $pad_bottom $pad_left $pad_right \
  --resize_factor $rescaleFactor \
  --fps $fps \
  --face_det_batch_size $face_det_batch_size \
  --wav2lip_batch_size $wav2lip_batch_size \
  --nosmooth


#@markdown <br><br> <b> Finally after LipSync, we Speed Down the video by the factor, we Sped Up with to get the desired video</b>
!ffmpeg -i /content/Wav2Lip/results/result_voice.mp4 -filter:v "setpts=2.0*PTS" -filter:a "atempo=0.5" -c:v libx264 -preset veryfast -crf 18 /content/Wav2Lip/results/answer.mp4

#Preview output video
clear_output()
print("Final Video Preview")
print("Download this video from", '/content/Wav2Lip/results/answer.mp4')
showVideo('/content/Wav2Lip/results/answer.mp4')

showVideo('/content/Wav2Lip/results/answer1.mp4')